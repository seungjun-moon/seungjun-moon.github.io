---
# multilingual page pair id, this must pair with translations of this page. (This name must be unique)
lng_pair: id_vae
title: "Variational AutoEncoder 이해하기 [1]"

# post specific
# if not specified, .name will be used from _data/owner/[language].yml
author: Mr. Green's Workshop
# multiple category is not supported
category: jekyll
# multiple tag entries are possible
tags: [jekyll, sample, example post]
use_math: true
# thumbnail image for post
img: ":post_pic1.jpg"
# disable comments on this page
#comments_disable: true

# publish date
date: 2023-07-21 08:11:06 +0900

# seo
# if not specified, date will be used.
#meta_modify_date: 2022-02-10 08:11:06 +0900
# check the meta_common_description in _data/owner/[language].yml
#meta_description: ""

# optional
# please use the "image_viewer_on" below to enable image viewer for individual pages or posts (_posts/ or [language]/_posts folders).
# image viewer can be enabled or disabled for all posts using the "image_viewer_posts: true" setting in _data/conf/main.yml.
#image_viewer_on: true
# please use the "image_lazy_loader_on" below to enable image lazy loader for individual pages or posts (_posts/ or [language]/_posts folders).
# image lazy loader can be enabled or disabled for all posts using the "image_lazy_loader_posts: true" setting in _data/conf/main.yml.
#image_lazy_loader_on: true
# exclude from on site search
#on_site_search_exclude: true
# exclude from search engines
#search_engine_exclude: true
# to disable this page, simply set published: false or delete this file
#published: false
---

<!-- outline-start -->

Diffusion 기반 생성 모델이 최근 크게 주목 받고 있다. Diffusion 모델을 공부하다 보면 피해갈 수 없는 게 바로 Variational AutoEncoder (VAE) 모델이다. Diffusion 모델을 제대로 이해하기 위해 최근 뒤늦게나마 VAE를 공부했다. 앞으로의 포스팅을 통해 VAE의 아이디어와 학습 방법 등을 **최대한 수식을 배제하고** 간단하게 설명해보려 한다. 이번 포스팅에서는 VAE의 등장 배경과 아이디어를 다뤄보려 한다.

<!-- outline-end -->

***

## VAE의 목적
여느 생성 모델과 마찬가지로 VAE 또한 실제 데이터와 유사한 데이터를 생성하는 것을 목표로 한다. 이를 위한 가장 간단하게는 아래와 같은 구조를 생각해볼 수 있을 것이다.

![Alt text](/assets/img/posts/vae1/structure1.png){: width="50%"",height="50%""}{:data-align="center"}

Latent $z$는 generator $g_{\theta}$를 거쳐서 생성 결과 $g_{\theta}(z)$를 만들어낸다. 우리의 목적은 학습 데이터 속 $X$의 확률을 최대화하는 것이다. 모든 $z$를 고려한 $X$의 확률은 아래와 같다.

<div align="center">
$P(X)=\int P(X|z;\theta)P(z)dz$
</div>

$z$와 $\theta$가 주어지면 네트워크의 출력값은 deterministic한데, $P(X\|z;\theta)$라는 개념은 어떻게 이해하면 될까? VAE(뿐만 아니라 AutoEncoder 역시)의 출력 값이(실제와는 다르게) 아래와 같은 Gaussian 분포(Non-deterministic)를 따른다고 가정하곤 한다.

<div align="center">
$P(X|z;\theta)=\mathcal{N}(X|g_{\theta}(z), \sigma^{2}*I)$
</div>

풀어서 설명하면, $z$가 주어졌을 때 출력값 $X$의 확률 분포는 네트워크의 출력 값 $g_{\theta}(z)$을 평균으로 하고, $\sigma^{2}\*I$을 분산으로 가지는 Gaussian 분포를 따른다는 뜻이다. 하지만 **네트워크의 출력 값은 $g_{\theta}(z)$로 deterministic하게 정해지지 않는가?** 물론 그렇지만, 학습을 위해 이를 변형시켜 생각하는 것이다. VAE는 학습 초반에(아마 후반에도) 주어진 학습 sample과 완전히 똑같은 출력 값을 만들진 못한다. 우리는 VAE가 gradient descent를 통해 $X$와 유사한 값을 만들어 가도록 적절한 loss를 디자인해줘야 한다. 그러나 네트워크의 출력 값이 deterministic하다고 해서, $P(X\|z;\theta)$의 확률 분포를 [Dirac delta 함수](https://en.wikipedia.org/wiki/Dirac_delta_function)로 가정하면 likelihood를 최대화하기 위한 gradient descent를 행할 수 없다. 따라서 네트워크의 출력 값이 Gaussian 분포를 따른다고 가정하는 것이고, 이 때 likelihood는 Euclidean distance가 작을수록 높게 나타난다. 물론 꼭 Gaussian 분포로 가정할 필요는 없다.

***

## Latent Sampling 방식의 문제점

다시 $P(X)$를 보자. $z$를 $n$번 sampling하여 $P(X)$를 근사하면 $P(X)\approx\sum_{i}P(X\|g_{\theta}(z_{i}))p(z_{i})\approx\frac{1}{n}\sum_{i}P(X\|g_{\theta}(z_{i}))$ 가 된다.
앞서 언급했듯, 이 때 likelihood는 Euclidean distance에 반비례한다. 사실 이렇게 되면 문제가 생기는데, 아래 그림을 보자.

![Alt text](/assets/img/posts/vae1/motivation.png){: width="70%""}{:data-align="center"}

(a)와 같은 학습 데이터를 활용하여 모델을 학습시켰다고 가정하자. 이 때, 샘플 (b)는 (a)의 글자에서 일부 픽셀을 지워낸 형태고, 샘플 (c)는 (a)의 픽셀을 조금씩 이동시킨 형태이다. 우리가 보기에는 (c)가 실제 데이터(숫자 2)에 가깝지만, (b)와 (a) 사이의 Euclidean distance가 (c)와 (a) 사이의 거리보다 훨씬 작다. 즉, (c)의 likelihood가 (b)에 비해 훨씬 높게 측정된다는 것이다. 이는 당연히 바람직하지 않은 모델이다. 이 문제는 $z$를 uniform하게 sampling하여 $\sum_{i}P(X\|g_{\theta}(z_{i}))p(z_{i})\approx\frac{1}{n}\sum_{i}P(X\|g_{\theta}(z_{i})$ 처럼 근사가 된 것이 원인이다. 즉, 우리는 **조금 더 똑똑하게 $z$를 sampling할 필요가 있다.**
여기서 VAE의 아이디어가 출발한다.

> $X$를 보여줄테니 $X$와 유의미하게 유사한 sample을 만들 수 있는 확률분포 $p(z\|X)$를 만들고, 여기에서 sampling을 하자.

즉, 기존처럼 완전히 독립적인 분포 $p(z)$에서부터 $z$를 sampling하는 것이 아닌, $X$에 따라 변하는 $z$의 분포에서 sampling하는 것이 VAE의 핵심이다. 이러한 이상적인 sampling 함수를 만들기 위해 우리는 posterior, 즉 encoder $q_{\phi}$를 학습하게 된다.

![Alt text](/assets/img/posts/vae1/structure2.png){: width="70%""}{:data-align="center"}

즉, VAE는 sampling을 영리하게 하자는 아이디어에서 출발해서 어쩌다보니 AutoEncoder와 비슷한 구조를 가지게 된 것이지, 근본적으로 그 출발점은 AutoEncoder와 굉장히 다르다. 다음 포스팅에서는 VAE를 어떻게 학습시키는지에 대해서 알아볼 것이다.