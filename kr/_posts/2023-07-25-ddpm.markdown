---
# multilingual page pair id, this must pair with translations of this page. (This name must be unique)
lng_pair: id_ddpm
title: "[논문 리뷰] Denoising Diffusion Probabilistic Models 이해하기"

# post specific
# if not specified, .name will be used from _data/owner/[language].yml
author: SeungJun Moon
# multiple category is not supported
category: jekyll
# multiple tag entries are possible
tags: [diffusion]
use_math: true
# thumbnail image for post
img: "/assets/img/posts/ddpm/structure.png"
# disable comments on this page
#comments_disable: true

# publish date
date: 2023-07-25 13:57:06 +0900

# seo
# if not specified, date will be used.
#meta_modify_date: 2022-02-10 08:11:06 +0900
# check the meta_common_description in _data/owner/[language].yml
#meta_description: ""

# optional
# please use the "image_viewer_on" below to enable image viewer for individual pages or posts (_posts/ or [language]/_posts folders).
# image viewer can be enabled or disabled for all posts using the "image_viewer_posts: true" setting in _data/conf/main.yml.
#image_viewer_on: true
# please use the "image_lazy_loader_on" below to enable image lazy loader for individual pages or posts (_posts/ or [language]/_posts folders).
# image lazy loader can be enabled or disabled for all posts using the "image_lazy_loader_posts: true" setting in _data/conf/main.yml.
#image_lazy_loader_on: true
# exclude from on site search
#on_site_search_exclude: true
# exclude from search engines
#search_engine_exclude: true
# to disable this page, simply set published: false or delete this file
# published: false
---

<!-- outline-start -->

지난 [VAE 포스팅](https://seungjun-moon.github.io/kr/2023-07-21-vae1)에 이어서 이번 포스팅에서는 [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239), 흔히 DDPM으로 널리 알려진 diffusion 모델에 대해서 설명하려고 한다.

<!-- outline-end -->

***

## DDPM과 VAE

VAE와 DDPM 모두 학습 데이터 $X$를 기반으로 latent를 추출하고, 이를 바탕으로 데이터를 재구성해나간다는 점에서 유사한 구조를 가지고 있습니다. 아마 이 두 모델의 가장 큰 차이점은 latent를 얻는 데에 한 step만을 필요로 하는지(VAE), 또는 여러 step을 거쳐 점진적으로 만들어 나가는지(DDPM) 일겁니다. $X$를 입력으로 받아 직접적으로 $\mu$와 $\Sigma$를 출력하여 latent의 분포를 예측하는 VAE와 다르게, DDPM은 $X$에 점진적으로 noise를 입히는 과정을 통해서 latent를 만들어냅니다. 그 과정이 어떻게 이루어지는지 아래 그림을 보면서 다시 설명드리겠습니다.

![Alt text](/assets/img/posts/ddpm/structure.png){: width="90%""}{:data-align="center"}

DDPM은 입력 이미지 $x_{0}$로 부터 시작해서 우선 noise를 입혀나가는 과정인 **forward process**를 거치게 됩니다. 이 때 총 $T$번의 스텝을 통해 noise가 충분히 입혀지게 되고, $x_{T}$는 Gaussian noise가 됩니다. 우리는 Gaussian noise, 즉 $x_{T}$로 부터 noise를 걷어내는 **reverse process**를 학습시키는 것이 목표입니다.

## Forward Process

Forward process는 $t-1$번째 step의 이미지 $x_{t-1}$에 Gaussian noise를 더하여 $x_{t}$를 만들어가는 과정입니다. 이 때 각 단계에서의 process는 바로 이전 단계의 상태에만 영향을 받는 [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain)이 되도록 설계합니다. Forward process $q(x_{t}\|x_{t-1})$는 아래와 같이 나타낼 수 있습니다.

<div align="center">
$q(x_{t}|x_{t-1})=\mathcal{N}(x_{t};\sqrt{1-\beta_{t}}x_{t-1},\beta_{t}\text{I})$
<!-- $q(x_{t}|x_{t-1})=\prod_{t=1}^{T}q(x_{t}|x_{t-1})$ -->
</div>

우리는 reparameterization trick을 활용해 $x_{t}=\sqrt{1-\beta_{t}}x_{t-1}+\sqrt{\beta_{t}}\cdot \epsilon_{t-1}, \epsilon\in\mathcal{N}(0,I)$로 나타낼 수 있습니다. VAE와 마찬가지로, 이런 식으로 sampling이라는 미분 불가능한 연산을 우회해서 DDPM의 loss를 계산할 때 활용할 수 있는 형태로 $x_{t}$를 바꿔줄 수 있습니다.

$\beta_{t}$는 주입되는 noise의 크기라고 생각하시면 됩니다. $\beta_{t}$의 경우 reparameterization을 활용하여 학습을 통해서 얻을 수도 있지만, DDPM 저자들은 fixed constant를 써도 상관없다고 합니다. DDPM에서는 실제로 $\beta_{t}$를 hyperparameter로 미리 지정해주기 때문에, forward process에서는 학습할 parameter가 없습니다. 저자들은 step을 거듭할수록 $\beta_{t}$가 점진적으로 커지도록 설정해줬습니다. 이 때, $\beta_{t}$가 실제 data scale보다 훨씬 작은 값을 가진다면, 우리는 $x_{t}$를 구하기 위해 $t$번의 noise 더하기를 하지 않고, 아래와 같은 식으로 한 번에 $x_{t}$를 구할 수 있음이 증명되어 있습니다.

<div align="center">
$q(x_{t}|x_{0})=\mathcal{N}(x_{t};\sqrt{\bar{\alpha}_t}x_{0}, (1-\bar{\alpha}_t)\cdot\text{I})$
</div>

여기서 $\alpha_{t}:=1-\beta_{t}$, $\bar{\alpha_{t}}:=\Pi_{s=1}^t\alpha_{s}$ 를 뜻합니다.
실제로 저자들은 데이터의 scale $[-1,1]$에 비해 훨씬 작은 $\beta_{t}$를 설정하여 ($\beta_{1}=10^{-4}$ to $\beta_{T}=0.02$) 위 식을 활용하였습니다.

## Reverse Process

Forward Process에서 이미지에 noise를 씌워서 Gaussian noise로 만들었으니, 이제는 noise를 제거해나가며 원래 이미지로 복원할 차례입니다. Noise를 제거하는 과정 역시 Gaussian noise를 더해간다는 점에서 forward process와 유사하지만, 올바르게 noise를 제거하기 위해선 **올바른** Gaussian noise를 더해줘야 할 것입니다. 우리는 이 올바른 Gaussian noise를 구하기 위해서 neural network $\theta$를 학습한다고 해도 무방합니다. 이 과정을 식으로 나타내면 다음과 같습니다.

<div align="center">
$p_{\theta}(x_{t-1}|x_{t})=\mathcal{N}(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x_{t},t))$
</div>

즉, 우리는 network를 통해서 reverse process에 쓰일 Gaussian noise의 평균과 분산 값을 예측하는 것이 목표라 할 수 있습니다.

## Training DDPM

DDPM 역시 많은 생성 모델들처럼 likelihood를 maximize하는, negative log likelihood를 loss 함수로 활용하여 학습합니다. Negative log likelihood에 variational bound를 적용한 후, 수식을 요리조리 변형시키다 보면 아래와 같은 식을 얻을 수 있습니다.

<div align="center">
$\mathbb{E}[-\text{log}p_{\theta}(x_{0})]\leq $\mathbb{E}_q[D_{KL}(q(x_T|x_0)||p(x_T))+\sum_{t>1}D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_{t}))-\text{log}p_{\theta}(x_0|x_1)]$
</div>
